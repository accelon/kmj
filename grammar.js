/* generate grammar of word sequence*/
import { meta_sc, nodefs, writeChanged, readTextLines,readTextContent,patchBuf} from 'ptk/nodebundle.cjs';
import {eachDef, eachSentence,isIdenticalGrammar,isNormalDef,parseNormalDef,reuseLemmas} from './src/raw-format.js'; 
import {Lexicon} from './src/lexicon.js';
import {rawpatches,reuselemmadecomp} from './src/rawpatch.js'
import {pnpatches} from './src/pnpatch.js'
await nodefs; //export fs to global

const srcfolder='./raw/'; 
const desfolder='./grammar/';

if (!fs.existsSync(desfolder)) fs.mkdirSync(desfolder);
const bkid=process.argv[2]||'dn1';

const books=meta_sc.booksOf(bkid);
console.log('books',books)
//generated by lexicon.js
const SameAs=JSON.parse(readTextContent('sameas.json')); //有 "同上" 的lemma, lexicon.js 產生
const lexicon=new Lexicon( JSON.parse( readTextContent('lexicon.json')));

const ctx={lexicon,SameAs,pnlemmas:{},reuselemmadecomp};
const addLemmas=(id,lex)=>{
    if (!ctx.pnlemmas[id]) ctx.pnlemmas[id]=lex;
    else {
        let count=0,newid=id;
        while (ctx.pnlemmas[newid]) {
            newid=id+'.'+(++count);
        }
        ctx.pnlemmas[newid]=lex;
    }
}
//

books.forEach(book=>{	
	const lines=readTextLines(srcfolder+book+'.txt');
    ctx.fn=book;
	
	eachSentence(lines,ctx,(pn,pali,rawdefs)=>{
		let j=0, done =false;
		const locallex=[],lemmas=[];
		ctx.locallex=locallex;
		ctx.pn=pn;
		const patchkey=ctx.fn+'_'+pn;
		const patches=rawpatches[patchkey];
		if (patches) {
			const newrawdefs=patchBuf( rawdefs.join('\n'), patches, patchkey);
			if (newrawdefs!==rawdefs.join('\n')) {
				delete rawpatches[patchkey]
				rawdefs=newrawdefs.split('\n');
			} else {
				console.log('unable to patch',patchkey)
			}
		}

		for (let j=0;j<rawdefs.length;j++) { //預先建好 本段的 lexicon 
            if (isNormalDef(rawdefs[j])) {
                locallex[j]=parseNormalDef(rawdefs[j],ctx)[0];
            }
        }
        
		const identical=isIdenticalGrammar(rawdefs,ctx);
        if (identical) {
            const reuse=ctx.pnlemmas[ctx.fnpf+'_'+identical];
            if (reuse) {
                lemmas.push(...reuse);
                done=true;
            }
        }
        if (!done) while (j<rawdefs.length){
            const rawdef=rawdefs[j];
            if (isNormalDef(rawdef)) { 
                lemmas.push(locallex[j]);
            } else {//contain expansion
                rawdef.replace(/(\d+\-\d+)/g,(m,m1)=>{
                    let expand = ctx.pnlemmas[ctx.fnpf+'_'+m1];
					if (!expand){
						const pnpatch=pnpatches[patchkey];
						if (pnpatch && pnpatch[m1]){
							expand=ctx.pnlemmas[ctx.fnpf+'_'+ pnpatch[m1]];
							
						} 
                    }
					if (!expand) console.log(patchkey, 'has invalid pn',m1);
					else lemmas.push(...expand);
                });
                if (rawdef.indexOf('（同上）')>-1) {
                    //look ahead for vars //an3_15-39
                    j=reuseLemmas(rawdef.replace('（同上）',''),lemmas,j,ctx);
                }
            }
            j++;
        }

		addLemmas(ctx.fnpf+'_'+pn,lemmas);
		//console.log(pn,pali,rawdefs)
	});
});