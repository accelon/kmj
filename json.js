import {nodefs,writeChanged,filesFromPattern, readTextContent
    , readTextLines} from 'ptk/nodebundle.cjs';
import {Lexicon} from './src/lexicon.js';
import {TDenList,diffList} from 'ptk/nodebundle.cjs'
import {eachDef, eachSentence,isNormalDef,parseNormalDef,isIdenticalGrammar} from './src/raw-format.js'; 
import {breakCompound,matchCompound} from './src/compound.js'

await nodefs; //export fs to global
const srcfolder='./raw/'
const desfolder='./json/'
const pat=process.argv[2]||'dn?';
if (!fs.existsSync(desfolder)) fs.mkdirSync(desfolder);
const files=filesFromPattern( pat+'.txt' , srcfolder);
if (!files.length) console.log('missing pattern')

//generated by lexicon.js
const SameAs=JSON.parse(readTextContent('sameas.json')); //有 "同上" 的lemma, lexicon.js 產生
const lexicon=new Lexicon( JSON.parse( readTextContent('./lexicon.json')));
const ctx={SameAs,lexicon};

const pnlemmas={}; 
const addLemmas=(id,lex)=>{
    if (!pnlemmas[id]) pnlemmas[id]=lex;
    else {
        let count=0,newid=id;
        while (pnlemmas[newid]) {
            newid=id+'.'+(++count);
        }
        pnlemmas[newid]=lex;
    }
}
//todo 'anabhiratisaññaṃ': [ 'sabba+loke+anabhirati+saññaṃ' ],
//pali sabbaloke anabhiratisaññaṃ

const Decompose=new Lexicon();
const addDecompose=decomposes=>{
    decomposes.forEach(line=>{
        const [w,part]=line.split('\t');
        Decompose.addRawDef(w,part);
    });
}


const genDecomposes=true; //產生decomposes.txt ，供validdecompose.js找出可能的拆分
// ctx.knownDecompose=loadKnownDecompose( readTextLines('./knowndecompose.txt'));

files.forEach(fn=>{
    const out=[];
    const lines=readTextLines(srcfolder+fn);
    //lines.length=114;
    ctx.fn=fn;
    ctx.fnpf=fn.replace('.txt','');
    process.stdout.write('\r'+ctx.fn+'       ')
    eachSentence(lines,ctx,(pn,pali,rawdefs)=>{
        ctx.debug=pn=='2-1';
        const lemmas=[];
        let j=0, done =false;
        ctx.pn=pn;
        ctx.locallex=[];
        for (let j=0;j<rawdefs.length;j++) { //預先建好 本段的 lexicon 
            if (isNormalDef(rawdefs[j])) {
                locallex[j]=parseNormalDef(rawdefs[j],ctx)[0];
            }
        }
        const identical=isIdenticalGrammar(rawdefs,ctx);
        if (identical) {
            const reuse=pnlemmas[ctx.fnpf+'_'+identical];
            if (reuse) {
                lemmas.push(...reuse);
                done=true;
            }
        }
        if (!done) while (j<rawdefs.length){
            const rawdef=rawdefs[j];
            if (isNormalDef(rawdef)) { 
                lemmas.push(locallex[j]);
            } else {//contain expansion
                rawdef.replace(/(\d+\-\d+)/g,(m,m1)=>{
                    const expand = pnlemmas[ctx.fnpf+'_'+m1];
                    if (expand) {
                        lemmas.push(...expand)
                    } else {
                        console.log('wrong pn',m1)
                    }
                });
                if (rawdef.indexOf('（同上）')>-1) {
                    //look ahead for vars //an3_15-39
                    j=reuseLemmas(rawdef.replace('（同上）',''),lemmas,j,ctx);
                }
            }
            j++;
        }

        if (genDecomposes) {
            const decomposes=breakCompound(pali, lemmas.join(' '),ctx);
            if (decomposes) addDecompose(decomposes);            
        } else {
            matchCompound(pali,lemmas, lexicon, ctx);
        }

        addLemmas(ctx.fnpf+'_'+pn,lemmas);
        out.push([pn, pali, lemmas.join(' ') ]);
        pn='';
    })
    const outfn=desfolder+fn.replace('.txt','.json');
    const outcontent=JSON.stringify(out,'',' ');
    writeChanged(outfn,outcontent,true)
})

if (genDecomposes){
    Decompose.packRaw();
    const decomposes=JSON.stringify(Decompose.entries,'',' ' );
    writeChanged('decomposes.txt',decomposes,true)
}
